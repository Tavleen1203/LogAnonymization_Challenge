{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp8yQrSmK7KoMs9wn3YN92",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tavleen1203/LogAnonymization_Challenge/blob/main/CodingChallenge_LFX_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CODING CHALLENGE:ANONYMIZATION USING NLP**"
      ],
      "metadata": {
        "id": "y_9B-g6l-6dD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approaches Used**\n",
        "\n",
        "1. For this challenge I have used two approaches, firstly I have done a simple non-NLP approach of hiding PIIs to gain intuition on the project, I have simply identified the PIIs, and hidden them by using X symbols.\n",
        "\n",
        "2. Next, after gaining sense of the implementation I have moved to the NLP Code, the technique that I used here was text masking. For this, I have created a sensitive_info object, for reference. Then upon iterating over the data, each time a pattern of the object is encountered, the mask_text() function is called and it masks the text based on tag defined in the sensitive_info."
      ],
      "metadata": {
        "id": "0FkBHdna79q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Used: https://github.com/logpai/loghub/blob/master/OpenStack/OpenStack_2k.log_structured.csv"
      ],
      "metadata": {
        "id": "eXnA_PCW3Iuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. ANONYMIZATION INTUITION**"
      ],
      "metadata": {
        "id": "NKNuyzdW9qlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"data_log.csv\", delimiter=\",\")\n",
        "\n",
        "# Anonymizing feilds\n",
        "data['User'] = 'XXXXX'\n",
        "data['PID'] = 'XXXXX'\n",
        "data['Address'] = 'XXXXX'\n",
        "\n",
        "\n",
        "data.to_csv(\"anonymized_data_log.csv\", index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-o22axcT3Ofn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. TEXT MASKING**"
      ],
      "metadata": {
        "id": "aGEpR8_gKhpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def mask_text(text):\n",
        "\n",
        "    # Giving pattern to understand sensitive data\n",
        "    sensitive_info = {\n",
        "        'USER': ['calvisitor', 'authorMacBook-Pro'],\n",
        "        'IP_ADDRESS': ['10.105.160.95', '10.105.162.105']\n",
        "        # Add more sensitive information patterns as needed\n",
        "    }\n",
        "\n",
        "    # Tokenizing\n",
        "    for token, patterns in sensitive_info.items():\n",
        "        for pattern in patterns:\n",
        "            text = text.replace(pattern, f'<{token}>')\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"data_log.csv\", delimiter=\"\\t\")\n",
        "\n",
        "# Masking\n",
        "for column in data.select_dtypes(include='object'):\n",
        "    data[column] = data[column].apply(mask_text)\n",
        "\n",
        "\n",
        "data.to_csv(\"masked_data_log.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "A9I4Bvkg5Mhf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  **3. BUILDING AN ENSEMBLE MODEL TO TRY ANOMYZATION**"
      ],
      "metadata": {
        "id": "0fnRJhPIKu4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "import spacy\n",
        "import random\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# sample text\n",
        "text = \"My name is Tavleen, I live in Delhi and my number is 9811264475.\"\n",
        "\n",
        "\n",
        "number_regex = r'\\b\\d+\\b'\n",
        "\n",
        "def anonymize_numbers(text):\n",
        "    return re.sub(number_regex, 'XXXXX', text)\n",
        "\n",
        "\n",
        "def scramble_number(match):\n",
        "    number = match.group(0)\n",
        "\n",
        "    digits = list(number)\n",
        "\n",
        "    random.shuffle(digits)\n",
        "\n",
        "    return ''.join(digits)\n",
        "\n",
        "def anonymize_entities(text):\n",
        "    doc = nlp(text)\n",
        "    anonymized_text = []\n",
        "    for token in doc:\n",
        "        if token.ent_type_ == \"PERSON\":\n",
        "            anonymized_text.append(\"XXXXX\")\n",
        "        elif token.ent_type_ == \"LOC\":\n",
        "            anonymized_text.append(\"XXXXX\")\n",
        "        elif token.ent_type_ == \"NUM\":\n",
        "            anonymized_text.append(re.sub(number_regex, scramble_number, token.text))\n",
        "        else:\n",
        "            anonymized_text.append(token.text)\n",
        "    return \" \".join(anonymized_text)\n",
        "\n",
        "\n",
        "def anonymize(text):\n",
        "    text = anonymize_numbers(text)\n",
        "    text = anonymize_entities(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "anonymized_text = anonymize(text)\n",
        "print(anonymized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29vXda-GK2eZ",
        "outputId": "2fb1620f-f8c7-438b-e7a4-1e9c49fbb6a7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is XXXXX , I live in Delhi and my number is XXXXX .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTIONS FROM THE CHALLENGE**"
      ],
      "metadata": {
        "id": "1u-Pe7HJFI-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: Is it possible to anonymize the dataset?**\n",
        "\n",
        "A: Yes, a dataset can be anonymized, we can hide sensitive information. An observation I made while coding this was, sensitive information should not reach anybody that means our aim should be to have no human intervention while hiding this data. This can be achieved by predictive analysis and Natural Language Processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zot0DBwwBBYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: Does it ‘successfully’ anonymize?**\n",
        "\n",
        "A: To consider this process succesfull in its entirety, it is important to note that a model needs to be trained on a very large set of data. When considering anonymization, its applications can be in numerous fields, like banking, healthcare, internet, online shopping, reviews and so on. To be successful, the model must be trained on enough data to identify what could possibly be a PII, and then hide it. (using hashing, regex, etc)"
      ],
      "metadata": {
        "id": "-U0ZnfW3B_Fv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: How easy is it to use NLP?**\n",
        "\n",
        "A: NLP Applications on text anonymization can vary from simple masking to use of advanced libraries. But considering effectiveness over ease, NLP is a pretty good approach. Yet these are the potential issues that we may encounter while working:\n",
        "\n",
        "1. Slow performance while dealing with large datasets.\n",
        "2. Slow learning curve due to huge domain specific applications.\n",
        "3. Validation: we will have to employ some automated method that  could validate the models working on test data.\n",
        "\n"
      ],
      "metadata": {
        "id": "JpCO2xq2DuXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Does it make sense to use NLP?**\n",
        "\n",
        "A: Yes, use of NLP is a very intelligent approach while dealing with text anonymization. As sensitive data shall not reach anybody, using a Machine Learning approach can remove the scope of anybody interacting with a user's information. This can be a highly secure method. Once the model is trained, it can autonomously deal with the task at hand."
      ],
      "metadata": {
        "id": "l3TH6NmuFUp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: Are the available libraries good enough?**\n",
        "\n",
        "A: Some available libraries such as spacy, NLTK, regex can be extented to the task of anonymization. Yet, the available libraried fail to completely align with this task.\n",
        "\n",
        "Here are the problems I faced while working on this challenge:\n",
        "\n",
        "1. Accuracy Issues: I initially started with more complex NLP Solutions like usage of spacy and textBlob, but what I obsereved was inability to handle variations, which required a lot of manual review.\n",
        "\n",
        "2. Another thing is that we need a long chain of ensemble processes. What I mean is in a dataset where let's say we have phone number and name, we need regex for the number, Named Entity Recognition (using spacy, textBlob etc) for the name, and then valdation strategies to check correctness of output, and then we can say that anonymization is done."
      ],
      "metadata": {
        "id": "vTOmFzzZGrVa"
      }
    }
  ]
}